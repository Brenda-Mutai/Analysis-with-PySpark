{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importint Relevant Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a varied number of options that you can configure when setting up a SparkSession. Let's go over a few of the more common ones:\n",
    "* **master**: The URL for the cluster SparkContext to connect to master: The URL for the cluster SparkContext to connect to\n",
    "* **appName**: The name that will be displayed in the Spark cluster UI\n",
    "* **config**: Configuration for SparkSession. Any key-value pairs in the config will be applied to the session's SparkConf. For example, you can set the spark.sql.shuffle.partitions configuration property to change the number of partitions in joins and aggregations. Or you can set spark.executor.memory to change the amount of memory used per executor process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session\n",
    "# Wrapping the session in brackets allows us to chain commands without using a \"\\\" in Python\n",
    "spark = (SparkSession.builder\n",
    "        .master(\"local[*]\") # Using all cores\n",
    "        .appName(\"Catch-Up Session\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Catch-Up Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2004bcf0be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the Spark Session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkSession is the entry point to Spark SQL. It manages the SparkContext that was used to create it, and provides a way to create DataFrames and DataSets. Spark SQL is the Spark module for working with structured data. It allows you to use SQL or the DataFrame/Dataset API to express Spark operations on structured data.\n",
    "\n",
    "For this exercise, we will be going over how we can read, access and manipulate structured data in Spark using Spark SQL. The data we will be using will be stored in the data directory contained in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of similarities between Pandas and Spark SQL when it comes to reading files. The main difference is that Spark SQL is able to read files from a distributed file system, such as HDFS, whereas Pandas is only able to read files from a local file system. Spark SQL is also able to read files from a local file system, but it is not recommended to do so in a production environment.\n",
    "\n",
    "On top of that, there are also similar functionalities when handling the dataframes produced. Let's explore some of those while also listing the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names to use in the wine dataset\n",
    "colNames = [\n",
    "    'target', 'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols',\n",
    "    'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue',\n",
    "    'od280_od315_of_diluted_wines', 'proline'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a variable called wine\n",
    "wine = spark.read.csv('./data/wine.data', header=False, inferSchema=True).toDF(*colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the csv, we could have gone about it in different manners. For example:\n",
    "1. `spark.read.csv(\"data/airports.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")`\n",
    "\n",
    "    In this instance, we are chaining the options we want to set to the dataframe. This is a very common way of doing things in Spark SQL.\n",
    "2. `spark.read.options(header=\"true\", inferSchema=\"true\").csv(\"data/airports.csv\")`\n",
    "\n",
    "    In this instance, we are passing the options as keyword arguments to the options function. This is also a very common way of doing things in Spark SQL.\n",
    "\n",
    "Pick what you prefer and stick with it. The important thing is to be **consistent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+\n",
      "|target|alcohol|malic_acid| ash|alcalinity_of_ash|magnesium|total_phenols|flavanoids|nonflavanoid_phenols|proanthocyanins|color_intensity| hue|od280_od315_of_diluted_wines|proline|\n",
      "+------+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+\n",
      "|     1|  14.23|      1.71|2.43|             15.6|      127|          2.8|      3.06|                0.28|           2.29|           5.64|1.04|                        3.92|   1065|\n",
      "|     1|   13.2|      1.78|2.14|             11.2|      100|         2.65|      2.76|                0.26|           1.28|           4.38|1.05|                         3.4|   1050|\n",
      "|     1|  13.16|      2.36|2.67|             18.6|      101|          2.8|      3.24|                 0.3|           2.81|           5.68|1.03|                        3.17|   1185|\n",
      "|     1|  14.37|      1.95| 2.5|             16.8|      113|         3.85|      3.49|                0.24|           2.18|            7.8|0.86|                        3.45|   1480|\n",
      "|     1|  13.24|      2.59|2.87|             21.0|      118|          2.8|      2.69|                0.39|           1.82|           4.32|1.04|                        2.93|    735|\n",
      "+------+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 rows of the wine dataset\n",
    "wine.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema is a description of the structure of your data. It is a list of fields (columns) and their data types, nothing more. It does not contain any data itself. A schema can be applied to a DataFrame, which allows Spark to understand the data in that DataFrame. This allows Spark to run certain optimizations on the data, and allows Spark to compress the data when it is serialized and sent over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- malic_acid: double (nullable = true)\n",
      " |-- ash: double (nullable = true)\n",
      " |-- alcalinity_of_ash: double (nullable = true)\n",
      " |-- magnesium: integer (nullable = true)\n",
      " |-- total_phenols: double (nullable = true)\n",
      " |-- flavanoids: double (nullable = true)\n",
      " |-- nonflavanoid_phenols: double (nullable = true)\n",
      " |-- proanthocyanins: double (nullable = true)\n",
      " |-- color_intensity: double (nullable = true)\n",
      " |-- hue: double (nullable = true)\n",
      " |-- od280_od315_of_diluted_wines: double (nullable = true)\n",
      " |-- proline: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema of the wine dataset\n",
    "wine.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for duplicates\n",
    "assert wine.count() == wine.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+---+-----------------+---------+-------------+----------+--------------------+---------------+---------------+---+----------------------------+-------+\n",
      "|target|alcohol|malic_acid|ash|alcalinity_of_ash|magnesium|total_phenols|flavanoids|nonflavanoid_phenols|proanthocyanins|color_intensity|hue|od280_od315_of_diluted_wines|proline|\n",
      "+------+-------+----------+---+-----------------+---------+-------------+----------+--------------------+---------------+---------------+---+----------------------------+-------+\n",
      "|     0|      0|         0|  0|                0|        0|            0|         0|                   0|              0|              0|  0|                           0|      0|\n",
      "+------+-------+----------+---+-----------------+---------+-------------+----------+--------------------+---------------+---------------+---+----------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for nulls\n",
    "wine.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in wine.columns]).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Derivation for the above code can be found [here](https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe)\n",
    "\n",
    "**Explanation**: The code loops over all the columns and for each, it filters the null entries and returns them to the count function which tallies them up. Each result gets an alias of the column name and is then unioned with the previous result. The final result is a dataframe with the column names and the number of null entries for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: None of the columns have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<'count(CASE WHEN (target IS NULL) THEN target END) AS target'>,\n",
       " Column<'count(CASE WHEN (alcohol IS NULL) THEN alcohol END) AS alcohol'>,\n",
       " Column<'count(CASE WHEN (malic_acid IS NULL) THEN malic_acid END) AS malic_acid'>,\n",
       " Column<'count(CASE WHEN (ash IS NULL) THEN ash END) AS ash'>,\n",
       " Column<'count(CASE WHEN (alcalinity_of_ash IS NULL) THEN alcalinity_of_ash END) AS alcalinity_of_ash'>,\n",
       " Column<'count(CASE WHEN (magnesium IS NULL) THEN magnesium END) AS magnesium'>,\n",
       " Column<'count(CASE WHEN (total_phenols IS NULL) THEN total_phenols END) AS total_phenols'>,\n",
       " Column<'count(CASE WHEN (flavanoids IS NULL) THEN flavanoids END) AS flavanoids'>,\n",
       " Column<'count(CASE WHEN (nonflavanoid_phenols IS NULL) THEN nonflavanoid_phenols END) AS nonflavanoid_phenols'>,\n",
       " Column<'count(CASE WHEN (proanthocyanins IS NULL) THEN proanthocyanins END) AS proanthocyanins'>,\n",
       " Column<'count(CASE WHEN (color_intensity IS NULL) THEN color_intensity END) AS color_intensity'>,\n",
       " Column<'count(CASE WHEN (hue IS NULL) THEN hue END) AS hue'>,\n",
       " Column<'count(CASE WHEN (od280_od315_of_diluted_wines IS NULL) THEN od280_od315_of_diluted_wines END) AS od280_od315_of_diluted_wines'>,\n",
       " Column<'count(CASE WHEN (proline IS NULL) THEN proline END) AS proline'>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's important to break down what happened in the above line of code\n",
    "# Here is the output of the list comprehension used.\n",
    "[F.count(F.when(F.isnull(c), c)).alias(c) for c in wine.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Since we have no null or duplicate values, the work we need to do is minimal. Let's move on to the next step.\n",
    "Let's rename a column in the dataframe. Let's focus on the od280_od315_of_diluted_wines column. This column is a bit hard to read, so let's rename it to od280."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great example of withColumnRenamed in action\n",
    "wine = wine.withColumnRenamed('od280_od315_of_diluted_wines', 'od280')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to look up how to feature engineer using the `withColumn` function. It is a very useful function that you will be using a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Data Using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to create a temporary view of a DataFrame by calling the createOrReplaceTempView method on that DataFrame. This will register the DataFrame as a table in the catalog, which will allow you to run SQL queries on its data. This is a temporary view, so it will only exist for the duration of your SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|target|valueCounts|\n",
      "+------+-----------+\n",
      "|     1|         59|\n",
      "|     2|         71|\n",
      "|     3|         48|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of using views in Spark\n",
    "wine.createOrReplaceTempView('wine')\n",
    "\n",
    "# Query for unique values and their distiribution in the target column\n",
    "spark.sql(\"\"\"SELECT target, COUNT(*) AS valueCounts FROM wine\n",
    "          GROUP BY target\n",
    "          ORDER BY target\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Clearly there is a class imbalance in our targe variable especially within class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|avgAlcohol|\n",
      "+------+----------+\n",
      "|     1|     13.74|\n",
      "|     2|     12.28|\n",
      "|     3|     13.15|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average alcohol content by target rounded off to 2 decimal places\n",
    "spark.sql( \"\"\"\n",
    "          SELECT target, ROUND(AVG(alcohol), 2) AS avgAlcohol FROM wine\n",
    "          GROUP BY target\n",
    "          ORDER BY target\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am sure SQL querying brings in a familiarity to the data scientists who have worked with SQL databases. It is still recommended to use the DataFrame API for most of your data manipulation tasks, as it is **more flexible and less error-prone** than SQL queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|avgAlcohol|\n",
      "+------+----------+\n",
      "|     1|     13.74|\n",
      "|     2|     12.28|\n",
      "|     3|     13.15|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It's possible to do the same using the DataFrame API\n",
    "# Remember the paranthesis just allow us to chain commands without using a \"\\\"\n",
    "(wine.select(['target', 'alcohol']).groupBy('target').\n",
    " agg(F.round(F.avg('alcohol'), 2).alias('avgAlcohol'))\n",
    " .orderBy('target')).show()\n",
    "\n",
    "# the same as \n",
    "# wine.select(['target', 'alcohol']).groupBy('target').agg(F.round(F.avg('alcohol'), 2).alias('avgAlcohol')).orderBy('target').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
