{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for High-Level Spark API using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark **is a `distributed computing framework` that runs on a cluster of machines. The cluster is managed by a driver program that runs on a master node. The driver program is responsible for distributing the work to the worker nodes and collecting the results from them. The driver program is also responsible for storing the data in the cluster. The data is stored in a data structure called RDD (Resilient Distributed Dataset). RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, which means that once they are created, they cannot be changed. The only way to change an RDD is to create a new RDD from an existing one. RDDs are fault-tolerant, which means that if a node in the cluster fails, the data on that node can be reconstructed from the other nodes. RDDs are also lazy, which means that they are not evaluated until an action is performed on them. This allows Spark to optimize the execution of the RDDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/spark-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The Driver Program`\n",
    "This is a single process that creates work for the cluster.\n",
    "\n",
    "`Spark Context`\n",
    "This is the main entry point for Spark functionality. It represents a connection to a Spark cluster and can be used to create RDDs, accumulators and broadcast variables on that cluster. It communicates with the cluster manager to allocate resources across applications. It also keeps track of the cluster state: which nodes are alive, which are dead, which are running which tasks, etc. The Spark Context is created by the driver program and is used to create RDDs.\n",
    "\n",
    "`Executors`\n",
    "These are multiple processes throughout the cluster that do  the work in parallel. They are responsible for executing the tasks that the driver program assigns to them. They also store the data that the driver program assigns to them. The number of executors is determined by the cluster manager.\n",
    "\n",
    "The Driver creates hobs from the user's code. `Jobs` are computations that can be executed in parallel. The SC divides jobs into tasks to be executed in the cluster. Tasks from a given job operate on different data subsets called partitions and can be executed in parallel.\n",
    "\n",
    "A `worker` is a cluster node that can launch execute processes to run tasks. Each executor is alloted a set number of cores that can each run one task at a time. Increasing executors and cores increases cluster parallelism, but also increases the amount of memory used by the cluster.\n",
    "\n",
    "![](./images/spark-stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `stage` is a set of tasks within a job that can be completed on the current local data partition.\n",
    "\n",
    "A `shuffle` marks the boundary between stages. Subsequent tasks in later stages must wait for that stage to be completed creating a dependency graph. A shuffle is an expensive operation that involves moving data between executors. It is important to minimize the number of shuffles in a job.\n",
    "\n",
    "A shuffle is:\n",
    "    <li> *Costly*, requiring data serialization, disk and network I/O. </li>\n",
    "    <li> *Necessary* when an operation requres data outside the current partition of the task. e.g, Group By with a given key requiring scanning of all partitions.</li>\n",
    "\n",
    "When Spark performs a shuffle, it redistributes data across the cluster. I employ you to look at the workaround provided by Spark for shuffles.\n",
    "\n",
    "![](./images/spark-dag-rdd.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Relevant Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a varied number of options that you can configure when setting up a SparkSession. Let's go over a few of the more common ones:\n",
    "* **master**: The URL for the cluster SparkContext to connect to master: The URL for the cluster SparkContext to connect to\n",
    "* **appName**: The name that will be displayed in the Spark cluster UI\n",
    "* **config**: Configuration for SparkSession. Any key-value pairs in the config will be applied to the session's SparkConf. For example, you can set the spark.sql.shuffle.partitions configuration property to change the number of partitions in joins and aggregations. Or you can set spark.executor.memory to change the amount of memory used per executor process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Creating a Spark Session\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Wrapping the session in brackets allows us to chain commands without using a \"\\\" in Python\u001b[39;00m\n\u001b[0;32m      3\u001b[0m spark \u001b[39m=\u001b[39m (SparkSession\u001b[39m.\u001b[39;49mbuilder\n\u001b[0;32m      4\u001b[0m         \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m         \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mCatch-Up Session\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m----> 6\u001b[0m         \u001b[39m.\u001b[39;49mgetOrCreate())\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\java_gateway.py:99\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    968\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    972\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    973\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    974\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    975\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    976\u001b[0m                         errread, errwrite,\n\u001b[0;32m    977\u001b[0m                         restore_signals,\n\u001b[0;32m    978\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m    979\u001b[0m                         start_new_session)\n\u001b[0;32m    980\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\25471\\anaconda3\\envs\\spark-env\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1457\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1459\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1460\u001b[0m                              creationflags,\n\u001b[0;32m   1461\u001b[0m                              env,\n\u001b[0;32m   1462\u001b[0m                              cwd,\n\u001b[0;32m   1463\u001b[0m                              startupinfo)\n\u001b[0;32m   1464\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1472\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1473\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# Creating a Spark Session\n",
    "# Wrapping the session in brackets allows us to chain commands without using a \"\\\" in Python\n",
    "spark = (SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"Catch-Up Session\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Catch-Up Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25bc07a7e80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the Spark Session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkSession is the entry point to Spark SQL. It manages the SparkContext that was used to create it, and provides a way to create DataFrames and DataSets. Spark SQL is the Spark module for working with structured data. It allows you to use SQL or the DataFrame/Dataset API to express Spark operations on structured data.\n",
    "\n",
    "For this exercise, we will be going over how we can read, access and manipulate structured data in Spark using Spark SQL. The data we will be using will be stored in the data directory contained in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of similarities between Pandas and Spark SQL when it comes to reading files. The main difference is that Spark SQL is able to read files from a distributed file system, such as HDFS, whereas Pandas is only able to read files from a local file system. Spark SQL is also able to read files from a local file system, but it is not recommended to do so in a production environment.\n",
    "\n",
    "On top of that, there are also similar functionalities when handling the dataframes produced. Let's explore some of those while also listing the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names to use in the wine dataset\n",
    "colNames = [\n",
    "    'target', 'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols',\n",
    "    'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue',\n",
    "    'od280_od315_of_diluted_wines', 'proline'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a variable called wine\n",
    "wine = spark.read.csv('./data/wine.data', header=False, inferSchema=True).toDF(*colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the csv, we could have gone about it in different manners. For example:\n",
    "1. `spark.read.csv(\"data/airports.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")`\n",
    "\n",
    "    In this instance, we are chaining the options we want to set to the dataframe. This is a very common way of doing things in Spark SQL.\n",
    "2. `spark.read.options(header=\"true\", inferSchema=\"true\").csv(\"data/airports.csv\")`\n",
    "\n",
    "    In this instance, we are passing the options as keyword arguments to the options function. This is also a very common way of doing things in Spark SQL.\n",
    "\n",
    "Pick what you prefer and stick with it. The important thing is to be **consistent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+\n",
      "|target|alcohol|malic_acid| ash|alcalinity_of_ash|magnesium|total_phenols|flavanoids|nonflavanoid_phenols|proanthocyanins|color_intensity| hue|od280_od315_of_diluted_wines|proline|\n",
      "+------+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+\n",
      "|     1|  14.23|      1.71|2.43|             15.6|      127|          2.8|      3.06|                0.28|           2.29|           5.64|1.04|                        3.92|   1065|\n",
      "|     1|   13.2|      1.78|2.14|             11.2|      100|         2.65|      2.76|                0.26|           1.28|           4.38|1.05|                         3.4|   1050|\n",
      "|     1|  13.16|      2.36|2.67|             18.6|      101|          2.8|      3.24|                 0.3|           2.81|           5.68|1.03|                        3.17|   1185|\n",
      "|     1|  14.37|      1.95| 2.5|             16.8|      113|         3.85|      3.49|                0.24|           2.18|            7.8|0.86|                        3.45|   1480|\n",
      "|     1|  13.24|      2.59|2.87|             21.0|      118|          2.8|      2.69|                0.39|           1.82|           4.32|1.04|                        2.93|    735|\n",
      "+------+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the first 5 rows of the wine dataset\n",
    "wine.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema is a description of the structure of your data. It is a list of fields (columns) and their data types, nothing more. It does not contain any data itself. A schema can be applied to a DataFrame, which allows Spark to understand the data in that DataFrame. This allows Spark to run certain optimizations on the data, and allows Spark to compress the data when it is serialized and sent over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- malic_acid: double (nullable = true)\n",
      " |-- ash: double (nullable = true)\n",
      " |-- alcalinity_of_ash: double (nullable = true)\n",
      " |-- magnesium: integer (nullable = true)\n",
      " |-- total_phenols: double (nullable = true)\n",
      " |-- flavanoids: double (nullable = true)\n",
      " |-- nonflavanoid_phenols: double (nullable = true)\n",
      " |-- proanthocyanins: double (nullable = true)\n",
      " |-- color_intensity: double (nullable = true)\n",
      " |-- hue: double (nullable = true)\n",
      " |-- od280_od315_of_diluted_wines: double (nullable = true)\n",
      " |-- proline: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema of the wine dataset\n",
    "wine.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "assert wine.count() == wine.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+---+-----------------+---------+-------------+----------+--------------------+---------------+---------------+---+----------------------------+-------+\n",
      "|target|alcohol|malic_acid|ash|alcalinity_of_ash|magnesium|total_phenols|flavanoids|nonflavanoid_phenols|proanthocyanins|color_intensity|hue|od280_od315_of_diluted_wines|proline|\n",
      "+------+-------+----------+---+-----------------+---------+-------------+----------+--------------------+---------------+---------------+---+----------------------------+-------+\n",
      "|     0|      0|         0|  0|                0|        0|            0|         0|                   0|              0|              0|  0|                           0|      0|\n",
      "+------+-------+----------+---+-----------------+---------+-------------+----------+--------------------+---------------+---------------+---+----------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for nulls\n",
    "wine.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in wine.columns]).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Derivation for the above code can be found [here](https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe)\n",
    "\n",
    "**Explanation**: The code loops over all the columns and for each, it filters the null entries and returns them to the count function which tallies them up. Each result gets an alias of the column name and is then unioned with the previous result. The final result is a dataframe with the column names and the number of null entries for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: None of the columns have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<'count(CASE WHEN (target IS NULL) THEN target END) AS target'>,\n",
       " Column<'count(CASE WHEN (alcohol IS NULL) THEN alcohol END) AS alcohol'>,\n",
       " Column<'count(CASE WHEN (malic_acid IS NULL) THEN malic_acid END) AS malic_acid'>,\n",
       " Column<'count(CASE WHEN (ash IS NULL) THEN ash END) AS ash'>,\n",
       " Column<'count(CASE WHEN (alcalinity_of_ash IS NULL) THEN alcalinity_of_ash END) AS alcalinity_of_ash'>,\n",
       " Column<'count(CASE WHEN (magnesium IS NULL) THEN magnesium END) AS magnesium'>,\n",
       " Column<'count(CASE WHEN (total_phenols IS NULL) THEN total_phenols END) AS total_phenols'>,\n",
       " Column<'count(CASE WHEN (flavanoids IS NULL) THEN flavanoids END) AS flavanoids'>,\n",
       " Column<'count(CASE WHEN (nonflavanoid_phenols IS NULL) THEN nonflavanoid_phenols END) AS nonflavanoid_phenols'>,\n",
       " Column<'count(CASE WHEN (proanthocyanins IS NULL) THEN proanthocyanins END) AS proanthocyanins'>,\n",
       " Column<'count(CASE WHEN (color_intensity IS NULL) THEN color_intensity END) AS color_intensity'>,\n",
       " Column<'count(CASE WHEN (hue IS NULL) THEN hue END) AS hue'>,\n",
       " Column<'count(CASE WHEN (od280_od315_of_diluted_wines IS NULL) THEN od280_od315_of_diluted_wines END) AS od280_od315_of_diluted_wines'>,\n",
       " Column<'count(CASE WHEN (proline IS NULL) THEN proline END) AS proline'>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's important to break down what happened in the above line of code\n",
    "# Here is the output of the list comprehension used.\n",
    "[F.count(F.when(F.isnull(c), c)).alias(c) for c in wine.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Since we have no null or duplicate values, the work we need to do is minimal. Let's move on to the next step.\n",
    "Let's rename a column in the dataframe. Let's focus on the od280_od315_of_diluted_wines column. This column is a bit hard to read, so let's rename it to od280."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great example of withColumnRenamed in action\n",
    "wine = wine.withColumnRenamed('od280_od315_of_diluted_wines', 'od280')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to look up how to feature engineer using the `withColumn` function. It is a very useful function that you will be using a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Data Using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to create a temporary view of a DataFrame by calling the createOrReplaceTempView method on that DataFrame. This will register the DataFrame as a table in the catalog, which will allow you to run SQL queries on its data. This is a temporary view, so it will only exist for the duration of your SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|target|valueCounts|\n",
      "+------+-----------+\n",
      "|     1|         59|\n",
      "|     2|         71|\n",
      "|     3|         48|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of using views in Spark\n",
    "wine.createOrReplaceTempView('wine')\n",
    "\n",
    "# Query for unique values and their distiribution in the target column\n",
    "spark.sql(\"\"\"SELECT target, COUNT(*) AS valueCounts FROM wine\n",
    "          GROUP BY target\n",
    "          ORDER BY target\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Clearly there is a class imbalance in our targe variable especially within class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|avgAlcohol|\n",
      "+------+----------+\n",
      "|     1|     13.74|\n",
      "|     2|     12.28|\n",
      "|     3|     13.15|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average alcohol content by target rounded off to 2 decimal places\n",
    "spark.sql( \"\"\"\n",
    "          SELECT target, ROUND(AVG(alcohol), 2) AS avgAlcohol FROM wine\n",
    "          GROUP BY target\n",
    "          ORDER BY target\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am sure SQL querying brings in a familiarity to the data scientists who have worked with SQL databases. It is still recommended to use the DataFrame API for most of your data manipulation tasks, as it is **more flexible and less error-prone** than SQL queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|avgAlcohol|\n",
      "+------+----------+\n",
      "|     1|     13.74|\n",
      "|     2|     12.28|\n",
      "|     3|     13.15|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It's possible to do the same using the DataFrame API\n",
    "# Remember the paranthesis just allow us to chain commands without using a \"\\\"\n",
    "(wine.select(['target', 'alcohol']).groupBy('target').\n",
    " agg(F.round(F.avg('alcohol'), 2).alias('avgAlcohol'))\n",
    " .orderBy('target')).show()\n",
    "\n",
    "# the same as \n",
    "# wine.select(['target', 'alcohol']).groupBy('target').agg(F.round(F.avg('alcohol'), 2).alias('avgAlcohol')).orderBy('target').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have very little preprocessing done, all we need to do is create a pipeline that will take in the data and output a model. We will be using the `VectorAssembler` to create a vector of all the features and then we will be using the `RandomForestClassifier` model to train our data.\n",
    "\n",
    "It would also be a good idea to cross validate our model to ensure that we are not overfitting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train, test = wine.randomSplit([0.7, 0.3], seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=wine.columns[1:], outputCol='features',\n",
    "                            handleInvalid='error')\n",
    "\n",
    "# Instantiate the RandomForestClassifier with the following parameters\n",
    "rfModel = RandomForestClassifier(featuresCol='features',\n",
    "                       labelCol='target', predictionCol='prediction',\n",
    "                       numTrees=50, maxDepth=5, seed=23\n",
    "                       )\n",
    "\n",
    "# Instantiate the Evaluation metric\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='target', predictionCol='prediction',\n",
    "                                              metricName='f1') # Using F1 score as the metric\n",
    "\n",
    "pipe = Pipeline(stages=[assembler, rfModel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the training data is 1.0.\n",
      "The F1 score on the test data is 0.9440014765596161.\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the training data\n",
    "pipeModel = pipe.fit(train)\n",
    "\n",
    "# Transform the training and test sets\n",
    "trainResults = pipeModel.transform(train)\n",
    "testResults = pipeModel.transform(test)\n",
    "\n",
    "# Use the evaluator to get the F1 score for the training and test sets\n",
    "trainF1 = evaluator.evaluate(trainResults)\n",
    "print(\"The F1 score on the training data is {}.\".format(trainF1))\n",
    "testF1 = evaluator.evaluate(testResults)\n",
    "print(\"The F1 score on the test data is {}.\".format(testF1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the size of the dataset, it's no surprise we have such good metrics. It would be a good idea to cross validate our model to ensure that we are not overfitting our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramater Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(rfModel.numTrees, [50, 100, 150])\\\n",
    "    .addGrid(rfModel.maxDepth, [5, 10, 15])\\\n",
    "    .build()\n",
    "    \n",
    "# Instantiate the CrossValidator with 5 folds/iterations\n",
    "cv = CrossValidator(estimator=pipe, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the cross validator to the training data\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9710135566188198,\n",
       " 0.962531413761677,\n",
       " 0.962531413761677,\n",
       " 0.9691582325720256,\n",
       " 0.9691582325720256,\n",
       " 0.9691582325720256,\n",
       " 0.939889764928558,\n",
       " 0.939889764928558,\n",
       " 0.939889764928558]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average F1 metric from the cross validator models.\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty descent performance. The `cvModel` variable is now saved as the best performing model. We can now use this model to make predictions on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine our best model as of Cross Validation\n",
    "bestRfModel = cvModel.bestModel.stages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the parameters we focused on, what are the best values?\n",
    "bestRfModel.getNumTrees, bestRfModel.getMaxDepth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
